{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions in Deep Learning\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. They are crucial components that:\n",
    "* Add non-linearity to the network, allowing it to learn complex patterns\n",
    "* Help normalize the output of each neuron\n",
    "* Enable the network to learn and make sense of complicated, non-linear relationships in the data\n",
    "\n",
    "Without activation functions, neural networks would just be linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:18:52.278253Z",
     "iopub.status.busy": "2025-03-16T10:18:52.278078Z",
     "iopub.status.idle": "2025-03-16T10:18:54.073558Z",
     "shell.execute_reply": "2025-03-16T10:18:54.072994Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate input values for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:18:54.075705Z",
     "iopub.status.busy": "2025-03-16T10:18:54.075277Z",
     "iopub.status.idle": "2025-03-16T10:18:54.077984Z",
     "shell.execute_reply": "2025-03-16T10:18:54.077555Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create three common activation functions:\n",
    "1. ReLU (Rectified Linear Unit): Most commonly used, computationally efficient\n",
    "2. Sigmoid: Used in binary classification output layers\n",
    "3. Tanh: Similar to sigmoid but with output range [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:18:54.079592Z",
     "iopub.status.busy": "2025-03-16T10:18:54.079408Z",
     "iopub.status.idle": "2025-03-16T10:18:54.082340Z",
     "shell.execute_reply": "2025-03-16T10:18:54.081896Z"
    }
   },
   "outputs": [],
   "source": [
    "relu = np.maximum(0, x)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and save the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:18:54.084004Z",
     "iopub.status.busy": "2025-03-16T10:18:54.083671Z",
     "iopub.status.idle": "2025-03-16T10:18:54.386390Z",
     "shell.execute_reply": "2025-03-16T10:18:54.385889Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(x, relu)\n",
    "plt.title('ReLU')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(x, sigmoid)\n",
    "plt.title('Sigmoid')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(x, tanh)\n",
    "plt.title('Tanh')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('activation_functions.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
