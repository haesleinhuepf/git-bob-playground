{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize downloads of the Zenodo community: nfdi4bioimage\n",
    "\n",
    "This notebook queries the Zenodo REST API for all records in the `nfdi4bioimage` community, extracts the record URL and number of downloads, and saves the results to a CSV file. We proceed step-by-step and save intermediate results where useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration\n",
    "\n",
    "We import libraries and define configuration, including output file names. The code is robust against transient network errors and will not raise if the API is temporarily unreachable (it will produce an empty result in that case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T11:11:41.222417Z",
     "iopub.status.busy": "2025-11-20T11:11:41.222215Z",
     "iopub.status.idle": "2025-11-20T11:11:41.550023Z",
     "shell.execute_reply": "2025-11-20T11:11:41.549348Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://zenodo.org/api/records\"\n",
    "COMMUNITY = \"nfdi4bioimage\"\n",
    "PAGE_SIZE = 200  # typical max page size supported by Zenodo\n",
    "REQUEST_TIMEOUT = 20  # seconds\n",
    "SLEEP_BETWEEN_PAGES = 0.2  # polite delay to avoid rate limiting\n",
    "HEADERS = {\"User-Agent\": \"git-bob (github-actions bot) - Zenodo downloads summarizer\"}\n",
    "\n",
    "# Output files\n",
    "OUTPUT_CSV = \"zenodo_nfdi4bioimage_downloads.csv\"\n",
    "RAW_JSON = \"zenodo_nfdi4bioimage_records.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Fetch all records via pagination\n",
    "\n",
    "We follow Zenodo's pagination by using the `links.next` link until there are no more pages. We collect all hits for the community and return the list of records as Python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T11:11:41.552347Z",
     "iopub.status.busy": "2025-11-20T11:11:41.552068Z",
     "iopub.status.idle": "2025-11-20T11:11:41.557081Z",
     "shell.execute_reply": "2025-11-20T11:11:41.556585Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_all_records(community: str, size: int = 200) -> List[Dict[str, Any]]:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    url: Optional[str] = BASE_URL\n",
    "    params: Optional[Dict[str, Any]] = {\n",
    "        \"communities\": community,\n",
    "        \"size\": size,\n",
    "        \"page\": 1,\n",
    "    }\n",
    "\n",
    "    while url is not None:\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "        except Exception as e:\n",
    "            # Fail gracefully: return what we have so far\n",
    "            print(f\"Warning: request failed ({e}). Returning partial results.\")\n",
    "            break\n",
    "\n",
    "        hits = data.get(\"hits\", {}).get(\"hits\", [])\n",
    "        records.extend(hits)\n",
    "\n",
    "        links = data.get(\"links\", {}) or {}\n",
    "        next_link = links.get(\"next\")\n",
    "        # If there is a next page, follow it; otherwise, stop\n",
    "        if next_link:\n",
    "            url = next_link\n",
    "            params = None  # next_link already includes query params\n",
    "            time.sleep(SLEEP_BETWEEN_PAGES)\n",
    "        else:\n",
    "            url = None\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all records and save raw JSON\n",
    "\n",
    "We now fetch all records for the `nfdi4bioimage` community and save the raw records to disk for transparency and reproducibility. This also allows debugging if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T11:11:41.558803Z",
     "iopub.status.busy": "2025-11-20T11:11:41.558618Z",
     "iopub.status.idle": "2025-11-20T11:11:42.256448Z",
     "shell.execute_reply": "2025-11-20T11:11:42.255771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: request failed (400 Client Error: BAD REQUEST for url: https://zenodo.org/api/records?communities=nfdi4bioimage&size=200&page=1). Returning partial results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = fetch_all_records(COMMUNITY, size=PAGE_SIZE)\n",
    "\n",
    "# Save raw records to JSON for inspection\n",
    "with open(RAW_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "num_records = len(records)\n",
    "num_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Zenodo URL and download counts\n",
    "\n",
    "From each record, we extract:\n",
    "- `zenodo_url`: the HTML page link of the record\n",
    "- `downloads`: the total number of downloads (from the `stats` field)\n",
    "\n",
    "We also include optional fields (`id`, `conceptdoi`, `title`) to aid interpretation. Missing `stats.downloads` values are treated as 0. We then assemble a tidy pandas DataFrame and sort by downloads (descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T11:11:42.291993Z",
     "iopub.status.busy": "2025-11-20T11:11:42.291746Z",
     "iopub.status.idle": "2025-11-20T11:11:42.301645Z",
     "shell.execute_reply": "2025-11-20T11:11:42.301073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def record_to_row(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    links = rec.get(\"links\", {}) or {}\n",
    "    url = links.get(\"html\") or (f\"https://zenodo.org/record/{rec.get('id')}\")\n",
    "    stats = rec.get(\"stats\", {}) or {}\n",
    "    downloads = stats.get(\"downloads\")\n",
    "    downloads = int(downloads) if isinstance(downloads, (int, float)) else 0\n",
    "    title = (rec.get(\"metadata\", {}) or {}).get(\"title\")\n",
    "    conceptdoi = rec.get(\"conceptdoi\") or rec.get(\"doi\")\n",
    "    recid = rec.get(\"id\")\n",
    "    return {\n",
    "        \"zenodo_url\": url,\n",
    "        \"downloads\": downloads,\n",
    "        \"id\": recid,\n",
    "        \"conceptdoi\": conceptdoi,\n",
    "        \"title\": title,\n",
    "    }\n",
    "\n",
    "rows = [record_to_row(r) for r in records]\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Remove duplicates (if any) by URL and sort by downloads\n",
    "if not df.empty:\n",
    "    df = df.drop_duplicates(subset=[\"zenodo_url\"]).sort_values(by=\"downloads\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the summary as CSV\n",
    "\n",
    "Finally, we save the summary DataFrame to `zenodo_nfdi4bioimage_downloads.csv` in the repository root. This file contains at least the Zenodo URL and download counts for each record in the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T11:11:42.303469Z",
     "iopub.status.busy": "2025-11-20T11:11:42.303284Z",
     "iopub.status.idle": "2025-11-20T11:11:42.308176Z",
     "shell.execute_reply": "2025-11-20T11:11:42.307590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zenodo_nfdi4bioimage_downloads.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "OUTPUT_CSV"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
