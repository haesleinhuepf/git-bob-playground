{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aaf39dd",
   "metadata": {},
   "source": [
    "# Activation Functions Plots\n",
    "\n",
    "In this notebook, we will generate plots for three different activation functions commonly used in neural networks:\n",
    "- Linear Activation\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "\n",
    "Each plot will be saved as a .png file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731328b",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, we need to import necessary libraries for plotting and handling numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591d74a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T14:23:43.887036Z",
     "iopub.status.busy": "2024-10-29T14:23:43.887036Z",
     "iopub.status.idle": "2024-10-29T14:23:44.206860Z",
     "shell.execute_reply": "2024-10-29T14:23:44.206860Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28bf18",
   "metadata": {},
   "source": [
    "## Define the Linear Activation Function and Plot\n",
    "\n",
    "The linear function is simply `f(x) = x`. Let's plot this for a range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f03f64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T14:23:44.209889Z",
     "iopub.status.busy": "2024-10-29T14:23:44.208889Z",
     "iopub.status.idle": "2024-10-29T14:23:44.289968Z",
     "shell.execute_reply": "2024-10-29T14:23:44.289739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the range for x\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Linear activation function\n",
    "linear_activation = x\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, linear_activation, label='Linear Activation')\n",
    "plt.title('Linear Activation Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.axhline(0, color='black',linewidth=0.5)\n",
    "plt.axvline(0, color='black',linewidth=0.5)\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)\n",
    "plt.legend()\n",
    "plt.savefig('linear_activation.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b929e72",
   "metadata": {},
   "source": [
    "## Define the Sigmoid Activation Function and Plot\n",
    "\n",
    "The sigmoid function is defined as `f(x) = 1 / (1 + exp(-x))`. Let's plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a511ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T14:23:44.292682Z",
     "iopub.status.busy": "2024-10-29T14:23:44.291654Z",
     "iopub.status.idle": "2024-10-29T14:23:44.351284Z",
     "shell.execute_reply": "2024-10-29T14:23:44.350408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "sigmoid_activation = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, sigmoid_activation, label='Sigmoid Activation')\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.axhline(0.5, color='red',linewidth=0.5, linestyle='--')\n",
    "plt.axvline(0, color='black',linewidth=0.5)\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)\n",
    "plt.legend()\n",
    "plt.savefig('sigmoid_activation.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec4df3",
   "metadata": {},
   "source": [
    "## Define the ReLU Activation Function and Plot\n",
    "\n",
    "The ReLU function is defined as `f(x) = max(0, x)`. Let's plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881c53d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T14:23:44.353813Z",
     "iopub.status.busy": "2024-10-29T14:23:44.352824Z",
     "iopub.status.idle": "2024-10-29T14:23:44.415984Z",
     "shell.execute_reply": "2024-10-29T14:23:44.415984Z"
    }
   },
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "relu_activation = np.maximum(0, x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, relu_activation, label='ReLU Activation')\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.axhline(0, color='black',linewidth=0.5)\n",
    "plt.axvline(0, color='black',linewidth=0.5)\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)\n",
    "plt.legend()\n",
    "plt.savefig('relu_activation.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f92823",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We have successfully created and saved plots of three activation functions: Linear, Sigmoid, and ReLU. These plots can now be used in a PowerPoint presentation to explain the concepts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
